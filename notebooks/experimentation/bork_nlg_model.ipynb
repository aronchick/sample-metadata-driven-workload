{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "# Where Is natural language going next??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "tags": [
     "skip"
    ]
   },
   "source": [
    "# B0RK - The Next Generation in Natural Language Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Input, Embedding, Dense, LSTM, Bidirectional\n",
    "from keras.layers import CuDNNLSTM, concatenate, Reshape, SpatialDropout1D\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from notebooks.experimentation.AttentionWeightedAverage import AttentionWeightedAverage\n",
    "\n",
    "\n",
    "def textgenrnn_model(num_classes, cfg, context_size=None,\n",
    "                     weights_path=None,\n",
    "                     dropout=0.0,\n",
    "                     optimizer=RMSprop(lr=4e-3, rho=0.99)):\n",
    "    '''\n",
    "    Builds the model architecture for textgenrnn and\n",
    "    loads the specified weights for the model.\n",
    "    '''\n",
    "\n",
    "    input = Input(shape=(cfg['max_length'],), name='input')\n",
    "    embedded = Embedding(num_classes, cfg['dim_embeddings'],\n",
    "                         input_length=cfg['max_length'],\n",
    "                         name='embedding')(input)\n",
    "\n",
    "    if dropout > 0.0:\n",
    "        embedded = SpatialDropout1D(dropout, name='dropout')(embedded)\n",
    "\n",
    "    rnn_layer_list = []\n",
    "    for i in range(cfg['rnn_layers']):\n",
    "        prev_layer = embedded if i is 0 else rnn_layer_list[-1]\n",
    "        rnn_layer_list.append(new_rnn(cfg, i+1)(prev_layer))\n",
    "\n",
    "    seq_concat = concatenate([embedded] + rnn_layer_list, name='rnn_concat')\n",
    "    attention = AttentionWeightedAverage(name='attention')(seq_concat)\n",
    "    output = Dense(num_classes, name='output', activation='softmax')(attention)\n",
    "\n",
    "    if context_size is None:\n",
    "        model = Model(inputs=[input], outputs=[output])\n",
    "        if weights_path is not None:\n",
    "            model.load_weights(weights_path, by_name=True)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    else:\n",
    "        context_input = Input(\n",
    "            shape=(context_size,), name='context_input')\n",
    "        context_reshape = Reshape((context_size,),\n",
    "                                  name='context_reshape')(context_input)\n",
    "        merged = concatenate([attention, context_reshape], name='concat')\n",
    "        main_output = Dense(num_classes, name='context_output',\n",
    "                            activation='softmax')(merged)\n",
    "\n",
    "        model = Model(inputs=[input, context_input],\n",
    "                      outputs=[main_output, output])\n",
    "        if weights_path is not None:\n",
    "            model.load_weights(weights_path, by_name=True)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizer,\n",
    "                      loss_weights=[0.8, 0.2])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create a new LSTM layer per parameters. Unfortunately,\n",
    "each combination of parameters must be hardcoded.\n",
    "The normal LSTMs use sigmoid recurrent activations\n",
    "for parity with CuDNNLSTM:\n",
    "https://github.com/keras-team/keras/issues/8860\n",
    "'''\n",
    "\n",
    "\n",
    "def new_rnn(cfg, layer_num):\n",
    "    has_gpu = len(K.tensorflow_backend._get_available_gpus()) > 0\n",
    "    if has_gpu:\n",
    "        if cfg['rnn_bidirectional']:\n",
    "            return Bidirectional(CuDNNLSTM(cfg['rnn_size'],\n",
    "                                           return_sequences=True),\n",
    "                                 name='rnn_{}'.format(layer_num))\n",
    "\n",
    "        return CuDNNLSTM(cfg['rnn_size'],\n",
    "                         return_sequences=True,\n",
    "                         name='rnn_{}'.format(layer_num))\n",
    "    else:\n",
    "        if cfg['rnn_bidirectional']:\n",
    "            return Bidirectional(LSTM(cfg['rnn_size'],\n",
    "                                      return_sequences=True,\n",
    "                                      recurrent_activation='sigmoid'),\n",
    "                                 name='rnn_{}'.format(layer_num))\n",
    "\n",
    "        return LSTM(cfg['rnn_size'],\n",
    "                    return_sequences=True,\n",
    "                    recurrent_activation='sigmoid',\n",
    "                    name='rnn_{}'.format(layer_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import LearningRateScheduler, Callback\n",
    "from keras.models import Model, load_model\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils import Sequence\n",
    "from keras import backend as K\n",
    "from .utils import textgenrnn_encode_cat\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def generate_sequences_from_texts(texts, indices_list,\n",
    "                                  textgenrnn, context_labels,\n",
    "                                  batch_size=128):\n",
    "    is_words = textgenrnn.config['word_level']\n",
    "    is_single = textgenrnn.config['single_text']\n",
    "    max_length = textgenrnn.config['max_length']\n",
    "    meta_token = textgenrnn.META_TOKEN\n",
    "\n",
    "    if is_words:\n",
    "        new_tokenizer = Tokenizer(filters='', char_level=True)\n",
    "        new_tokenizer.word_index = textgenrnn.vocab\n",
    "    else:\n",
    "        new_tokenizer = textgenrnn.tokenizer\n",
    "\n",
    "    while True:\n",
    "        np.random.shuffle(indices_list)\n",
    "\n",
    "        X_batch = []\n",
    "        Y_batch = []\n",
    "        context_batch = []\n",
    "        count_batch = 0\n",
    "\n",
    "        for row in range(indices_list.shape[0]):\n",
    "            text_index = indices_list[row, 0]\n",
    "            end_index = indices_list[row, 1]\n",
    "\n",
    "            text = texts[text_index]\n",
    "\n",
    "            if not is_single:\n",
    "                text = [meta_token] + list(text) + [meta_token]\n",
    "\n",
    "            if end_index > max_length:\n",
    "                x = text[end_index - max_length: end_index + 1]\n",
    "            else:\n",
    "                x = text[0: end_index + 1]\n",
    "            y = text[end_index + 1]\n",
    "\n",
    "            if y in textgenrnn.vocab:\n",
    "                x = process_sequence([x], textgenrnn, new_tokenizer)\n",
    "                y = textgenrnn_encode_cat([y], textgenrnn.vocab)\n",
    "\n",
    "                X_batch.append(x)\n",
    "                Y_batch.append(y)\n",
    "\n",
    "                if context_labels is not None:\n",
    "                    context_batch.append(context_labels[text_index])\n",
    "\n",
    "                count_batch += 1\n",
    "\n",
    "                if count_batch % batch_size == 0:\n",
    "                    X_batch = np.squeeze(np.array(X_batch))\n",
    "                    Y_batch = np.squeeze(np.array(Y_batch))\n",
    "                    context_batch = np.squeeze(np.array(context_batch))\n",
    "\n",
    "                    # print(X_batch.shape)\n",
    "\n",
    "                    if context_labels is not None:\n",
    "                        yield ([X_batch, context_batch], [Y_batch, Y_batch])\n",
    "                    else:\n",
    "                        yield (X_batch, Y_batch)\n",
    "                    X_batch = []\n",
    "                    Y_batch = []\n",
    "                    context_batch = []\n",
    "                    count_batch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sequence(X, textgenrnn, new_tokenizer):\n",
    "    X = new_tokenizer.texts_to_sequences(X)\n",
    "    X = sequence.pad_sequences(\n",
    "        X, maxlen=textgenrnn.config['max_length'])\n",
    "\n",
    "    return X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
